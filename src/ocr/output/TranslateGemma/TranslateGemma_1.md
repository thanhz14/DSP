to greatly improve translation quality (Finkelstein et al., 2024). As the source of monolingual data we use the MADLAD-400 corpus (Kudugunta et al., 2023).

We aim to produce up to 10K synthetic examples per language pair. In order to select the source sentences that potentially benefit more from the synthetic data generation, we first bucket the original segments by length. We then sample each bucket to obtain 1 million source segments for each language pair we wish to generate synthetic data for. We then run a preliminary filtering step across these source segments where we take 2 samples from Gemini 2.5 Flash, once using greedy decoding and once sampled with a temperature of 1.0 and compare their scores according to MetricX 24-QE (Juraska et al., 2024). We select the sources where the sample achieves the largest improvement over the greedy decoding. The intuition behind this source filtering approach is that we wish to select sources that will benefit the most from 128-sample QE decoding, so we use 2 samples as a low-cost approximation.

After this selection process, for each of the sources for each language pair we generate 128 samples from Gemini 2.5 Flash and then apply a MetricX 24-QE filter to select the best-performing examples. We generate translations of two distinct lengths this way: individual sentences and text blobs of up to 512 tokens. This way we aim to support both translations of individual segments as well as longer texts. For generating these translations we used the same prompt as we used for further training (see Section 5.2). In order to avoid formatting issues or erroneous translations, we apply an additional formatting filtering step, again based on Gemini 2.5 Flash. This methodology was applied for all language pairs that are covered by WMT24++ (Deutsch et al., 2025) plus an additional set of 30 language pairs that are specified in Appendix B.

#### 2.2. Human-Generated Translation Data

To increase the diversity and script coverage of the data we also include data for additional lower-resource languages. For these languages, we opt to use human-generated parallel data instead. This data comes from the SMOL (Caswell et al., 2025) and GATITOS (Jones et al., 2023) datasets. SMOL covers 123 languages and GATITOS covers 170.



#### 2.3. Language distribution

The final proportion of languages for the SFT and RL phases can be found in Figure 1. For RL we used the same translation data as for SFT, except for GATITOS and SMOL that were used in SFT only. We provide the full list of languages that were included in training in Appendix C.

#### 2.4. Generic Instruction-Following Data

Our SFT mixture also includes 30% generic instruction-following data from the original Gemma 3 mixture. The purpose of including this data is to prevent the model from overfitting to the translation task and to maintain generic instruction-following capabilities.

### 3. Supervised Fine-Tuning

For supervised fine-tuning (SFT), we begin with the released Gemma 3 27B, 12B and 4B checkpoints. We use parallel data including both human-generated texts as well as synthetic data generated by Gemini (Gemini Team, 2025), as described in Section 2. In addition we use generic instruction-following data. We use the Kauldron SFT tooling $ ^{1} $ to fine-tune the Gemma 3 checkpoints. For fine-tuning we use the AdaFactor optimizer (Shazeer and Stern, 2018) with a learning rate of 0.0001 and a batch size of 64, running for 200k steps. We update all model parameters, but freeze the embedding parameters, as preliminary experiments indicated this helped with translation performance for languages and scripts not covered in the SFT data mix.

### 4. Reinforcement Learning

We performed reinforcement learning on top of the SFT checkpoint, using an ensemble of metrics