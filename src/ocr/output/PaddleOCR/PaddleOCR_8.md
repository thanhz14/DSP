### 3. Dataset

#### 3.1. Layout Analysis

To ensure robust model performance across diverse real-world document scenarios, we curate a in-house dataset for layout analysis. The data sources encompass 38k document images across diverse domains, including Academic papers, Textbooks, Market Analysis, Financial reports, Slides, Newspapers, Supplementary Teaching Materials, Examination Papers, and various Invoices and Receipts. The dataset features meticulous manual annotations across 25 distinct component categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Title, Display Formula, Table, Reference, Doc Title, Footnote, Header, Algorithm, Footer, Seal, Chart, Formula Number, Aside Text, Reference Content, Header Image, Footer Image, Inline Formula, Vertical Text, and Vision Footnote. All documents are manually annotated with element-level boundaries and their corresponding reading order, enabling effective training and evaluation for both layout element detection and reading order restoration. This high-quality ground truth ensures that the model can accurately reconstruct both the spatial structure and the logical flow of complex documents.

The data curation process incorporates specific data mining strategies aimed at expanding dataset diversity and identifying hard cases to improve model robustness. This workflow begins with clustering-based sampling applied to an extensive internal data pool, utilizing visual features to ensure a representative distribution and minimize redundancy. Subsequently, a hard-case mining pipeline is executed using PP-DocLayoutV2 [9] for dual-threshold inference. Samples exhibiting a significant discrepancy in detection density between high and low confidence thresholds are categorized as unstable cases. This methodology facilitates the systematic discovery of non-conventional layout structures—including comics, CAD drawings, and high-aspect ratio screenshots—which diverge from standard document formats. These instances are further refined through a human-in-the-loop process. Integrating these diverse scenarios into the dataset broadens the model's representation of characteristics and enhances its adaptive capacity in complex real-world document domains.

###### 3.2. PaddleOCR-VL-1.5-0.9B

The data construction strategy for PaddleOCR-VL-1.5-0.9B is driven by two core objectives: enhancing model robustness on challenging samples and expanding the breadth of supported capabilities. Consequently, our data preparation pipeline is divided into two distinct parts: (1) Hard Example Mining (Section 3.2.1), which focuses on identifying and weighting high-uncertainty samples to refine the model's decision boundaries; and (2) New Capability Data Construction (Section 3.2.2), which involves curating specialized datasets to unlock new skills such as text spotting, seal recognition, and advanced multilingual support.

##### 3.2.1. Data Selection Strategy: Uncertainty-Aware Cluster Sampling

To maximize the efficiency of the instruction fine-tuning stage (Stage 2), we propose a data curation strategy designed to balance visual diversity and sample difficulty. Instead of uniform random sampling, we employ an Uncertainty-Aware Cluster Sampling (UACS) mechanism. This approach ensures that the training data covers a wide spectrum of visual scenarios while allocating more training budget to "hard" cases where the model exhibits high uncertainty.

1. Visual Feature Clustering. First, to guarantee the diversity of visual layouts across the six tasks (OCR, Table, Formula, Chart, Seal, and Spotting), we utilize the CLIP [21] visual encoder to