##### 3.5.1. Training DeepEncoder

Following Vary [36], we utilize a compact language model [15] and use the next token prediction framework to train DeepEncoder. In this stage, we use all OCR 1.0 and 2.0 data aforementioned, as well as 100M general data sampled from the LAION [31] dataset. All data is trained for 2 epochs with a batch size of 1280, using the AdamW [23] optimizer with cosine annealing scheduler [22] and a learning rate of 5e-5. The training sequence length is 4096.

##### 3.5.2. Training DeepSeek-OCR

After DeepEncoder is ready, we use data mentioned in Section 3.4 to train the DeepSeek-OCR. with the entire training process conducted on the HAI-LLM [14] platform. The entire model uses pipeline parallelism (PP) and is divided into 4 parts, with DeepEncoder taking two parts and the decoder taking two parts. For DeepEncoder, we treat SAM and the compressor as the vision tokenizer, place them in PP0 and freeze their parameters, while treating the CLIP part as input embedding layer and place it in PP1 with unfrozen weights for training. For the language model part, since DeepSeek3B-MoE has 12 layers, we place 6 layers each on PP2 and PP3. We use 20 nodes (each with 8 A100-40G GPUs) for training, with a data parallelism (DP) of 40 and a global batch size of 640. We use the AdamW optimizer with a step-based scheduler and an initial learning rate of 3e-5. For text-only data, the training speed is 90B tokens/day, while for multimodal data, the training speed is 70B tokens/day.

<div style="text-align: center;">Table 2 | We test DeepSeek-OCR's vision-text compression ratio using all English documents with 600-1300 tokens from the Fox [21] benchmarks. Text tokens represent the number of tokens after tokenizing the ground truth text using DeepSeek-OCR's tokenizer. Vision Tokens=64 or 100 respectively represent the number of vision tokens output by DeepEncoder after resizing input images to 512×512 and 640×640.</div>



<table border=1 style='margin: auto; word-wrap: break-word;'><tr><td rowspan="2">Text Tokens</td><td colspan="2">Vision Tokens =64</td><td colspan="2">Vision Tokens=100</td><td rowspan="2">Pages</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>Precision</td><td style='text-align: center; word-wrap: break-word;'>Compression</td><td style='text-align: center; word-wrap: break-word;'>Precision</td><td style='text-align: center; word-wrap: break-word;'>Compression</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>600-700</td><td style='text-align: center; word-wrap: break-word;'>96.5%</td><td style='text-align: center; word-wrap: break-word;'>10.5\times</td><td style='text-align: center; word-wrap: break-word;'>98.5%</td><td style='text-align: center; word-wrap: break-word;'>6.7\times</td><td style='text-align: center; word-wrap: break-word;'>7</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>700-800</td><td style='text-align: center; word-wrap: break-word;'>93.8%</td><td style='text-align: center; word-wrap: break-word;'>11.8\times</td><td style='text-align: center; word-wrap: break-word;'>97.3%</td><td style='text-align: center; word-wrap: break-word;'>7.5\times</td><td style='text-align: center; word-wrap: break-word;'>28</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>800-900</td><td style='text-align: center; word-wrap: break-word;'>83.8%</td><td style='text-align: center; word-wrap: break-word;'>13.2\times</td><td style='text-align: center; word-wrap: break-word;'>96.8%</td><td style='text-align: center; word-wrap: break-word;'>8.5\times</td><td style='text-align: center; word-wrap: break-word;'>28</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>900-1000</td><td style='text-align: center; word-wrap: break-word;'>85.9%</td><td style='text-align: center; word-wrap: break-word;'>15.1\times</td><td style='text-align: center; word-wrap: break-word;'>96.8%</td><td style='text-align: center; word-wrap: break-word;'>9.7\times</td><td style='text-align: center; word-wrap: break-word;'>14</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>1000-1100</td><td style='text-align: center; word-wrap: break-word;'>79.3%</td><td style='text-align: center; word-wrap: break-word;'>16.5\times</td><td style='text-align: center; word-wrap: break-word;'>91.5%</td><td style='text-align: center; word-wrap: break-word;'>10.6\times</td><td style='text-align: center; word-wrap: break-word;'>11</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>1100-1200</td><td style='text-align: center; word-wrap: break-word;'>76.4%</td><td style='text-align: center; word-wrap: break-word;'>17.7\times</td><td style='text-align: center; word-wrap: break-word;'>89.8%</td><td style='text-align: center; word-wrap: break-word;'>11.3\times</td><td style='text-align: center; word-wrap: break-word;'>8</td></tr><tr><td style='text-align: center; word-wrap: break-word;'>1200-1300</td><td style='text-align: center; word-wrap: break-word;'>59.1%</td><td style='text-align: center; word-wrap: break-word;'>19.7\times</td><td style='text-align: center; word-wrap: break-word;'>87.1%</td><td style='text-align: center; word-wrap: break-word;'>12.6\times</td><td style='text-align: center; word-wrap: break-word;'>4</td></tr></table>

### 4. Evaluation

#### 4.1. Vision-text Compression Study

We select Fox [21] benchmarks to verify DeepSeek-OCR's compression-decompression capability for text-rich documents, in order to preliminarily explore the feasibility and boundaries of contexts optical compression. We use the English document portion of Fox, tokenize the ground truth text with DeepSeek-OCR's tokenizer (vocabulary size of approximately 129k), and select documents with 600-1300 tokens for testing, which happens to be 100 pages. Since the number of text tokens is not large, we only need to test performance in Tiny and Small modes, where Tiny mode corresponds to 64 tokens and Small mode corresponds to 100 tokens. We use the prompt