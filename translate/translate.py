# -*- coding: utf-8 -*-
"""translate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bP2nC6EPQuDreNbKts2tQFk6imOJ-32R
"""

from google.colab import drive
drive.mount('/content/drive')

output_path = "/content/drive/MyDrive/DSP_pr/2510.18234v1_vi.md"

# B∆∞·ªõc 1: C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install transformers accelerate torch sentencepiece bitsandbytes -q

from huggingface_hub import login
login(new_session=True)

##########################
#Cell load model
###########################
import warnings
warnings.filterwarnings('ignore')

# Import th∆∞ vi·ªán
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import gc

# Load model HY-MT1.5-7B
print("=" * 60)

print("L∆∞u √Ω: M√¥ h√¨nh l·ªõn, c√≥ th·ªÉ m·∫•t 5-10 ph√∫t ƒë·ªÉ t·∫£i")
print("=" * 60)

model_name = "tencent/HY-MT1.5-7B"
print("ƒêang t·∫£i m√¥ h√¨nh" ,model_name,"...")
# Load v·ªõi 4-bit quantization
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    load_in_4bit=True,
    trust_remote_code=True
)

print("‚úì Model ƒë√£ s·∫µn s√†ng!")
print(f"Device: {model.device}")
print(f"Memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
print("\n" + "=" * 60)
print("‚úÖ CELL 1 HO√ÄN TH√ÄNH - Model ƒë√£ ƒë∆∞·ª£c load v√†o b·ªô nh·ªõ")
print("B√¢y gi·ªù b·∫°n c√≥ th·ªÉ ch·∫°y CELL 2 bao nhi√™u l·∫ßn t√πy th√≠ch!")
print("=" * 60)

# ============================================================
# CELL 2: TRANSLATE
# ============================================================

import re
import torch
from google.colab import files

# ================= AUTOSAVE CONFIG =================
AUTOSAVE_EVERY = 5          # l∆∞u sau m·ªói 5 ƒëo·∫°n
AUTOSAVE_FILENAME = None    # s·∫Ω set sau khi upload
# ===================================================

# =================================================================
# C·∫§U H√åNH GENERATION - T√ôY CH·ªàNH THEO GPU
# =================================================================

GENERATION_CONFIG_A100 = {
    "max_new_tokens": 4096,
    "top_k": 20,
    "top_p": 0.6,
    "temperature": 0.7,
    "repetition_penalty": 1.05,
    "do_sample": True,
    "num_beams": 1,
}

GENERATION_CONFIG_T4 = {
    "max_new_tokens": 2048,
    "top_k": 20,
    "top_p": 0.6,
    "temperature": 0.7,
    "repetition_penalty": 1.05,
    "do_sample": True,
}

GENERATION_CONFIG_FREE = {
    "max_new_tokens": 4096,
    "top_k": 20,
    "top_p": 0.6,
    "temperature": 0.7,
    "repetition_penalty": 1.05,
    "do_sample": True,
}

# ===== CH·ªåN C·∫§U H√åNH =====
GENERATION_CONFIG = GENERATION_CONFIG_FREE

# ============================================================
# MARKDOWN HELPERS
# ============================================================

def extract_markdown_elements(text):
    elements = {
        'code_blocks': [],
        'inline_code': [],
        'latex_blocks': [],
        'latex_inline': [],
    }

    code_pattern = r'```[\s\S]*?```'
    for m in re.finditer(code_pattern, text):
        elements['code_blocks'].append(m.group(0))
    text = re.sub(code_pattern, lambda m: f'___CODE_BLOCK_{len(elements["code_blocks"])-1}___', text)

    latex_block_pattern = r'\$\$[\s\S]*?\$\$'
    for m in re.finditer(latex_block_pattern, text):
        elements['latex_blocks'].append(m.group(0))
    text = re.sub(latex_block_pattern, lambda m: f'___LATEX_BLOCK_{len(elements["latex_blocks"])-1}___', text)

    latex_inline_pattern = r'\$[^\$\n]+\$'
    for m in re.finditer(latex_inline_pattern, text):
        elements['latex_inline'].append(m.group(0))
    text = re.sub(latex_inline_pattern, lambda m: f'___LATEX_INLINE_{len(elements["latex_inline"])-1}___', text)

    inline_code_pattern = r'`[^`\n]+`'
    for m in re.finditer(inline_code_pattern, text):
        elements['inline_code'].append(m.group(0))
    text = re.sub(inline_code_pattern, lambda m: f'___INLINE_CODE_{len(elements["inline_code"])-1}___', text)
    print(f"   üîç Extracted: {len(elements['code_blocks'])} code blocks, "
          f"{len(elements['inline_code'])} inline codes, "
          f"{len(elements['latex_blocks'])} latex blocks")
    print(f"   üìù Processed text preview: {text[:100]}...")
    return text, elements


def restore_markdown_elements(text, elements):
    for i, v in enumerate(elements['code_blocks']):
        text = text.replace(f'___CODE_BLOCK_{i}___', v)
    for i, v in enumerate(elements['latex_blocks']):
        text = text.replace(f'___LATEX_BLOCK_{i}___', v)
    for i, v in enumerate(elements['latex_inline']):
        text = text.replace(f'___LATEX_INLINE_{i}___', v)
    for i, v in enumerate(elements['inline_code']):
        text = text.replace(f'___INLINE_CODE_{i}___', v)
    return text


def split_text_smart(text, max_words=300):
    sentences = re.split(r'(?<=[.!?])\s+(?=[A-Z])', text)
    chunks, cur = [], ""
    for s in sentences:
        if len(cur.split()) + len(s.split()) < max_words:
            cur += s + " "
        else:
            if cur.strip():
                chunks.append(cur.strip())
            cur = s + " "
    if cur.strip():
        chunks.append(cur.strip())
    ###########
    print(f"   ‚úÇÔ∏è  Split into {len(chunks)} chunks:")
    for i, c in enumerate(chunks):
        print(f"      Chunk {i}: {len(c.split())} words")
    return chunks if chunks else [text]

# ============================================================
# TRANSLATION
# ============================================================

def translate_text(text):
    print(f"English text{text}")
    prompt = f"Translate the following segment into Vietnamese, without additional explanation (if some part have this format, keep it intact because it is the metadata <|ref|>text<|/ref|><|det|>[[x,y,z,t]]<|/det|>.Do not hold special token, only return output token).\n\n{text}"
    messages = [{"role": "user", "content": prompt}]

    tokenized = tokenizer.apply_chat_template(
        messages,
        tokenize=True,
        add_generation_prompt=False,
        return_tensors="pt",
        return_token_type_ids=False
    )

    with torch.no_grad():
        outputs = model.generate(
            tokenized.to(model.device),
            **GENERATION_CONFIG,
            pad_token_id=tokenizer.eos_token_id
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if prompt in decoded:
        decoded = decoded.replace(prompt, "")
    decoded = decoded.replace("<|eos|>", "").strip()
    return decoded


def translate_markdown(markdown_text):
    paragraphs = markdown_text.split('\n\n')
    translated = []
    total = len(paragraphs)

    print(f"\n{'='*60}")
    print(f"T·ªïng s·ªë ƒëo·∫°n c·∫ßn d·ªãch: {total}")
    print(f"{'='*60}\n")

    for idx, para in enumerate(paragraphs):
        print(f"[{idx+1}/{total}]", end='')

        if not para.strip():
            translated.append(para)
            print(" ‚è≠Ô∏è")
            continue

        if para.strip().startswith('#'):
            m = re.match(r'^(#+)\s+(.+)$', para.strip())
            if m:
                translated.append(f"{m.group(1)} {translate_text(m.group(2))}")
                print(" ‚úì Header")
                continue

        processed, elements = extract_markdown_elements(para)
        chunks = split_text_smart(processed)
        #print(f"chunk after split{chunks}")
        out_chunks = []

        for c in chunks:
            try:
                #print(f"chunks {c}")
                out_chunks.append(translate_text(c))
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except Exception as e:
                print(" ‚ùå", e)
                out_chunks.append(c)
        print(f"output_chunk: {out_chunks}")
        final_para = restore_markdown_elements(' '.join(out_chunks), elements)
        translated.append(final_para)
        print(" ‚úì")

        # ================= AUTOSAVE =================
        if AUTOSAVE_FILENAME and (idx + 1) % AUTOSAVE_EVERY == 0:
            with open(AUTOSAVE_FILENAME, 'w', encoding='utf-8') as f:
                f.write('\n\n'.join(translated))
            print(f" üíæ Autosaved ({idx+1}/{total})")
        # ============================================

    return '\n\n'.join(translated)

# ============================================================
# UPLOAD FILE
# ============================================================

print("\n" + "="*60)
print("UPLOAD FILE MARKDOWN")
print("="*60)
uploaded = files.upload()

filename = list(uploaded.keys())[0]
AUTOSAVE_FILENAME = filename.replace('.md', '_autosave_vi.md')
print(f"üìå Autosave file: {AUTOSAVE_FILENAME}")

with open(filename, 'r', encoding='utf-8') as f:
    original_text = f.read()

# ============================================================
# TRANSLATE
# ============================================================

translated_text = translate_markdown(original_text)

# ============================================================
# SAVE FINAL FILES
# ============================================================

output_filename = filename.replace('.md', '_vi.md')
with open(output_filename, 'w', encoding='utf-8') as f:
    f.write(translated_text)

bilingual_filename = filename.replace('.md', '_bilingual.md')
with open(bilingual_filename, 'w', encoding='utf-8') as f:
    f.write("# B·∫£n d·ªãch song ng·ªØ\n\n")
    for o, t in zip(original_text.split('\n\n'), translated_text.split('\n\n')):
        f.write(f"**English:** {o}\n\n**Ti·∫øng Vi·ªát:** {t}\n\n---\n\n")

files.download(output_filename)
files.download(bilingual_filename)

print("\nüéâ HO√ÄN TH√ÄNH!")